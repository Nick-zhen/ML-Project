{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 5: Putting it all together \n",
    "### Associated lectures: All material till lecture 13\n",
    "\n",
    "#### Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330-2023s/blob/master/docs/calendar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "0. [Submission instructions](#si)\n",
    "1. [Understanding the problem](#1)\n",
    "2. [Data splitting](#2)\n",
    "3. [EDA](#3)\n",
    "4. (Optional) [Feature engineering](#4)\n",
    "5. [Preprocessing and transformations](#5) \n",
    "6. [Baseline model](#6)\n",
    "7. [Linear models](#7)\n",
    "8. [Different models](#8)\n",
    "9. (Optional) [Feature selection](#9)\n",
    "10. [Hyperparameter optimization](#10)\n",
    "11. [Interpretation and feature importances](#11) \n",
    "12. [Results on the test set](#12)\n",
    "13. [Summary of the results](#13)\n",
    "14. (Optional) [Your takeaway from the course](#15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submission instructions <a name=\"si\"></a>\n",
    "<hr>\n",
    "rubric={points:4}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. To submit this assignment, follow the instructions below:\n",
    "\n",
    "- **You may work on this assignment in a group (group size <= 4) and submit your assignment as a group.** \n",
    "- Below are some instructions on working as a group.  \n",
    "    - The maximum group size is 4. \n",
    "    - You can choose your own group members. \n",
    "    - Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "    - Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "    - It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. [Here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members) are some instructions on adding group members in Gradescope.  \n",
    "- Be sure to follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023s/blob/main/docs/homework_instructions.md).\n",
    "- Upload the .ipynb file to Gradescope.\n",
    "- **If the .ipynb file is too big or doesn't render on Gradescope for some reason, also upload a pdf or html in addition to the .ipynb.** \n",
    "- Make sure that your plots/output are rendered properly in Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group members:\n",
    "Nick Zheng, Keven Wen, Linda Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report final test score and metric:\n",
    "Our best pefroming model is LightGBM, considering aggregate performance on test score, recall and time. LightGBM provide test_score of 0.658 (+/- 0.005). Below is our confusion matrix for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import tests_hw5\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "\n",
    "In this homework you will be working on an open-ended mini-project, where you will put all the different things you have learned so far together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "1. This mini-project is open-ended, and while working on it, there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). Make sure you explain your decisions whenever necessary. \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "\n",
    "#### Assessment\n",
    "We plan to grade fairly and leniently. We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "\n",
    "#### A final note\n",
    "Finally, this style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (15-20 hours???) is a good guideline for this project . Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and I hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1. Pick your problem and explain the prediction problem <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:3}\n",
    "\n",
    "In this mini project, you will be working on a classification problem of predicting whether a credit card client will default or not. \n",
    "For this problem, you will use [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). In this data set, there are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default.payment.next.month\" in the data. The rest of the columns can be used as features. You may take some ideas and compare your results with [the associated research paper](https://www.sciencedirect.com/science/article/pii/S0957417407006719), which is available through [the UBC library](https://www.library.ubc.ca/). \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Spend some time understanding the problem and what each feature means. You can find this information in the documentation on [the dataset page on Kaggle](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). Write a few sentences on your initial thoughts on the problem and the dataset. \n",
    "2. Download the dataset and read it as a pandas dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is binary which is 1 for default, 0 for non-default. The features are almost nemeric. But for sex and marriage can be binary categorical features and education can be ordinal categorical feature. For those numeric features, I am planning to scale them. In the end, our goal is to predict whether a credic card client will default or not depending on those feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"data/UCI_Credit_Card.csv\")\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2. Data splitting <a name=\"2\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train (70%) and test (30%) portions with `random_state=123`.\n",
    "\n",
    "> If your computer cannot handle training on 70% training data, make the test split bigger.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "train_df, test_df = train_test_split(data_frame, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['EDUCATION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3. EDA <a name=\"3\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Perform exploratory data analysis on the train set.\n",
    "2. Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it.\n",
    "3. Summarize your initial observations about the data. \n",
    "4. Pick appropriate metric/metrics for assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.describe().loc['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing data from the above information and all features are float or int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.hist(bins=50, figsize=(20, 15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID is not relevant to our task, maybe we should drop it. And we see some features are mainly focus on 0 but has huge range such as PAY_AMT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"default.payment.next.month\"])\n",
    "y_train = train_df['default.payment.next.month']\n",
    "\n",
    "X_test = test_df.drop(columns=[\"default.payment.next.month\"])\n",
    "y_test = test_df['default.payment.next.month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cor = pd.concat((y_train, X_train), axis=1).iloc[:, :25].corr()\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.set(font_scale=.8)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like PAY_1,2,3,4,5,6 is highly correlated with each other and same as BILL_AMT1,2,3,4,5,6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Summarize initial observations about the data:\n",
    "- we have all numeric features.\n",
    "- But some features have huge range, we need to scale those numeric feature values.\n",
    "- we have high correlation in some features.\n",
    "- we have not unclear data such as education, how does the value in education featuer mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[\"default.payment.next.month\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have class imbalance. We have 77% non-default classes and 22% default class.\n",
    "4. we are planning to pick confusion matrix and macro recall to assessment. Because we think both classes are equally important and also false negative is more important than false positive in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## (Optional) 4. Feature engineering <a name=\"4\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out feature engineering. In other words, extract new features relevant for the problem and work with your new feature set in the following exercises. You may have to go back and forth between feature engineering and preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't think ID is important. We are planning to drop ID firstly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 5. Preprocessing and transformations <a name=\"5\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Identify different feature types and the transformations you would apply on each feature type. \n",
    "2. Define a column transformer, if necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_feats = ['AGE',\n",
    "                 'LIMIT_BAL', \n",
    "                 'PAY_0',\n",
    "                 'PAY_2',\n",
    "                 'PAY_3',\n",
    "                 'PAY_4',\n",
    "                 'PAY_5',\n",
    "                 'PAY_6',\n",
    "                 'BILL_AMT1',\n",
    "                 'BILL_AMT2',\n",
    "                 'BILL_AMT3',\n",
    "                 'BILL_AMT4',\n",
    "                 'BILL_AMT5',\n",
    "                 'BILL_AMT6',\n",
    "                 'PAY_AMT1',\n",
    "                 'PAY_AMT2',\n",
    "                 'PAY_AMT3',\n",
    "                 'PAY_AMT4',\n",
    "                 'PAY_AMT5',\n",
    "                 'PAY_AMT6']                   \n",
    "categorical_feats = ['MARRIAGE']\n",
    "ordinal_feats = ['EDUCATION']\n",
    "binary_feats = ['SEX']\n",
    "drop_feats = ['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (StandardScaler(), numeric_feats),\n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_feats),\n",
    "    (OrdinalEncoder(), ordinal_feats),\n",
    "    (OneHotEncoder(drop='if_binary', dtype=int, sparse=False), binary_feats),\n",
    "    (\"drop\", drop_feats)\n",
    ")\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_transformed = ct.fit_transform(X_train)\n",
    "column_names = (numeric_feats + \n",
    "                ct.named_transformers_['onehotencoder-1'].get_feature_names_out().tolist()\n",
    "                 + ordinal_feats + binary_feats)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_df = pd.DataFrame(X_transformed, columns=column_names)\n",
    "preprocess_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adapted from lecture 3\n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "    \"\"\"\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 6. Baseline model <a name=\"6\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try `scikit-learn`'s baseline model and report results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_6\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "target_score = make_scorer(recall_score, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adapted from lecture 7\n",
    "dummy = DummyClassifier()\n",
    "results[\"Dummy\"] = mean_std_cross_val_scores(dummy, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(results).T\n",
    "#our first baseline model is dummy classifier because our problem is a classification problem of predicting whether a credit card client will default or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy result shows that we have around 77% are non-default classes and 22% default class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[\"Dummy (recall)\"] = mean_std_cross_val_scores(dummy, X_train, y_train, scoring=target_score)\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 7. Linear models <a name=\"7\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try a linear model as a first real attempt. \n",
    "2. Carry out hyperparameter tuning to explore different values for the complexity hyperparameter. \n",
    "3. Report cross-validation scores along with standard deviation. \n",
    "4. Summarize your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_7\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adapted from lecture 7 and sklear.org\n",
    "lr = LogisticRegression(max_iter=1000) \n",
    "pipe_lr = make_pipeline(ct, lr)\n",
    "#using the default settings did not converge within the maximum number of iterations. Enorder to fix this we adjusted a hyperparameter that controls the maximum number of iterations\n",
    "#We have tried to optimize this but it gives warnings so we will just keep it as 1000\n",
    "results['logistic_regression'] = mean_std_cross_val_scores(pipe_lr, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results['logistic_regression (recall)'] = mean_std_cross_val_scores(pipe_lr, X_train, y_train, scoring=target_score)\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carry out hyperparameter tuning to explore different values for the complexity hyperparameter.\n",
    "from scipy.stats import uniform, loguniform\n",
    "param_lr = {\n",
    "    \"logisticregression__C\": loguniform(1e-3, 1e3), # uniform(0.001, 1e4), # , \n",
    "}\n",
    "search_lr = RandomizedSearchCV(pipe_lr, param_lr, n_iter=100, verbose=1, n_jobs=-1, random_state=321, scoring=target_score)\n",
    "\n",
    "search_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_C = search_lr.best_estimator_\n",
    "pd.DataFrame(search_lr.cv_results_).set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataframe above, it shows that the best C for this model is 206.8 . For convienient, We will chose C=200 as best hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_lr = LogisticRegression(max_iter=1000, C=200) \n",
    "best_pipe_lr = make_pipeline(ct, best_lr)\n",
    "results['logistic_regression (Best C)'] = mean_std_cross_val_scores(best_pipe_lr, X_train, y_train, scoring=target_score)\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are not much difference between default C and the best C. They are all 0.608. But the result is not so high, maybe we should try other non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adapted from lecture 7\n",
    "# we can also see intercept that was learned by the baseline model\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print(\"Model weights: %s\" % (pipe_lr.named_steps[\"logisticregression\"].coef_))  # these are the learned weights\n",
    "print(\"Model intercept: %s\" % (pipe_lr.named_steps[\"logisticregression\"].intercept_))  # this is the bias term\n",
    "data = {\"features\":preprocess_df.columns, \"coefficients\": pipe_lr.named_steps[\"logisticregression\"].coef_[0]}\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### interpret coefficients:\n",
    "Some interesting coefficients here age. Notice that we are predicting for credit card default so this positive coefficient is telling us increasing in age will slightly increase chances of default which is a bit counter intuitive because many people could consider young people to have less wealth and less care thus more likely to default. LIMIT_BAL is amount of given credit. In general, people who have more savings, income and history are granted more credits so this coefficient aligns with our judgment. The series of pay are positive because these pays are ordered by payment delay, so people who would delay payment more are more likely going to default. Interestingly, based on the coefficients, married people MARRIAGE_1 are more likely going to default than people with single status MARRIAGE_2 and being female (higher number in sex) means less likely to default. Lastly, being higher in education means less likely to default with a large coefficient (an important factor like sex,marriage,early payment and payment amount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 8. Different models <a name=\"8\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try at least 3 other models aside from a linear model. One of these models should be a tree-based ensemble model. \n",
    "2. Summarize your results in terms of overfitting/underfitting and fit and score times. Can you beat a linear model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_8\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adapted from lecture 11\n",
    "\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "pipe_dt = make_pipeline(ct, DecisionTreeClassifier(random_state=123))\n",
    "pipe_rf = make_pipeline(ct, RandomForestClassifier(random_state=123))\n",
    "pipe_lgbm = make_pipeline(ct, LGBMClassifier(random_state=123))\n",
    "\n",
    "classifiers = {\n",
    "    \"decision tree\": pipe_dt,\n",
    "    \"random forest\": pipe_rf,\n",
    "    \"LightGBM\": pipe_lgbm,\n",
    "}\n",
    "\n",
    "for (name, model) in classifiers.items():\n",
    "    results[name] = mean_std_cross_val_scores(\n",
    "        model, X_train, y_train, return_train_score=True, scoring=target_score\n",
    "    )\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 3 models: Decision tree, random forest, and lightGBM. All these models are tried with default hyperparameters. Random forest seem to be best performing model among the models. The best cv score is 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adapted from lecture 11: comparison of results (excluding the the 'Dummy' row [1:])\n",
    "\n",
    "cv_score_order = pd.DataFrame(results).T[1:].sort_values('test_score').index\n",
    "print('\\nCV scores:')\n",
    "print(*cv_score_order, sep=' < ')\n",
    "\n",
    "fit_time_order = pd.DataFrame(results).T[1:].sort_values('fit_time').index\n",
    "print('\\nFitting speeds:')\n",
    "print(*fit_time_order, sep=' > ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using three ensembles model: decision tree, random forest, and LightGBM. Among these models, decision tree and random forest are overfits. LightGBM seems to perform the best that it has the best fitting speed while less overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## (Optional) 9. Feature selection <a name=\"9\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to select relevant features. You may try `RFECV` or forward selection for this. Do the results improve with feature selection? Summarize your results. If you see improvements in the results, keep feature selection in your pipeline. If not, you may abandon it in the next exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_9\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 10. Hyperparameter optimization <a name=\"10\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to optimize hyperparameters for the models you've tried and summarize your results. In at least one case you should be optimizing multiple hyperparameters for a single model. You may use `sklearn`'s methods for hyperparameter optimization or fancier Bayesian optimization methods. \n",
    "  - [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)   \n",
    "  - [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  - [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_10\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid_dt = {\n",
    "    \"decisiontreeclassifier__max_depth\": range(1, 20),\n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(pipe_dt, param_grid=param_grid_dt, cv=5, n_jobs=-1, scoring=target_score)\n",
    "dt_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"decision tree best max depth: \", dt_grid_search.best_params_)\n",
    "dt_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    \"randomforestclassifier__n_estimators\": range(3, 50),\n",
    "    \"randomforestclassifier__max_depth\": range(2, 15),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rd_random_search = RandomizedSearchCV(pipe_rf, param_grid_rf, n_iter=50, verbose=1, n_jobs=-1, scoring=target_score, random_state=123)\n",
    "rd_random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"random forest tree best hyperparameters: \", rd_random_search.best_params_)\n",
    "rd_random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid_lgbm = {\n",
    "    \"lgbmclassifier__n_estimators\": range(10, 300),\n",
    "    \"lgbmclassifier__max_depth\": range(3, 5, 7),\n",
    "    \"lgbmclassifier__learning_rate\": [0.01, 0.1, 1],\n",
    "    \"lgbmclassifier__subsample\": [0.5, 0.8, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_random_search = RandomizedSearchCV(pipe_lgbm, param_grid_lgbm, n_iter=50, verbose=1, n_jobs=-1, scoring=target_score, random_state=123)\n",
    "lgbm_random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LGBM best hyperparameters: \", lgbm_random_search.best_params_)\n",
    "lgbm_random_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 11. Interpretation and feature importances <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Use the methods we saw in class (e.g., `eli5`, `shap`) (or any other methods of your choice) to examine the most important features of one of the non-linear models. \n",
    "2. Summarize your observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_11\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried the random forest classifier in this question because this model has highest score in last question. However, it took a lot of time to calculate SHAP value. Therefore, we choose light GBM to do this question because light GBM also has a great score compare to random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adapted from lecture 12\n",
    "pipe_lgbm.fit(X_train, y_train)\n",
    "\n",
    "eli5_rf = eli5.explain_weights(\n",
    "    pipe_lgbm.named_steps[\"lgbmclassifier\"], feature_names=column_names\n",
    ")\n",
    "eli5_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above figure, it shows the feature improtances for random forest classifier. Those values tell us globally about which features are important. And the most important feature is **PAY_0** which is 0.4038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "import shap\n",
    "warnings.simplefilter(action=\"default\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(\n",
    "    data=X_transformed,\n",
    "    columns=column_names,\n",
    "    index=X_train.index,\n",
    ")\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_explainer = shap.TreeExplainer(pipe_lgbm.named_steps[\"lgbmclassifier\"])\n",
    "train_lgbm_shap_values = lgbm_explainer.shap_values(X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lgbm_shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgbm_shap_values[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "X_train_enc = round(X_train_enc, 3)  # for better visualization\n",
    "# X_train_enc.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    train_lgbm_shap_values[1][0, :],\n",
    "    X_train_enc.iloc[0, :],\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw model score for the first train data sample is much less than the base value. It means the predict of first train sample is non-default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adapted from lecture 12\n",
    "values = np.abs(train_lgbm_shap_values[1]).mean(\n",
    "    0\n",
    ")  # mean of shapely values in each column\n",
    "pd.DataFrame(data=values, index=column_names, columns=[\"SHAP\"]).sort_values(\n",
    "    by=\"SHAP\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adapted from lecture 12\n",
    "shap.summary_plot(train_lgbm_shap_values[1], X_train_enc, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(train_lgbm_shap_values[1], X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above plots, \n",
    "1. it seems that PAY_0, LIMIT_BAL, BILL_AMT1 are some important feature.\n",
    "2. Presence of PAY_* seems to have larger SHAP values for default.\n",
    "3. low education seems to have smaller SHAP value for default, which does not make a lot of sense. There might be some education level not be treated properly in preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 12. Results on the test set <a name=\"12\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try your best performing model on the test data and report test scores. \n",
    "2. Do the test scores agree with the validation scores from before? To what extent do you trust your results? Do you think you've had issues with optimization bias? \n",
    "3. Take one or two test predictions and explain these individual predictions (e.g., with SHAP force plots).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_12\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = make_pipeline(ct, LGBMClassifier(\n",
    "            subsample=0.5,\n",
    "            n_estimators=209,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.1\n",
    "        )\n",
    "    )\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy_score = best_model.score(X_test, y_test)\n",
    "test_recall_score = recall_score(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The test accuracy score: %f\" % test_accuracy_score)\n",
    "print(\"The test recall score: %f\" % test_recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test validation score (0.66) agrees with the validation scores from before. We are 80% trust out results. And we think we do not have optimization bias because our dataset is not too small and the validation sets are not hitting too many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adapted from lecture 12\n",
    "X_test_enc = pd.DataFrame(\n",
    "    data=ct.transform(X_test),\n",
    "    columns=column_names,\n",
    "    index=X_test.index,\n",
    ")\n",
    "X_test_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lgbm_shap_values = lgbm_explainer.shap_values(X_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_enc = round(X_test_enc, 3)  # for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(X_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    test_lgbm_shap_values[1][1, :],\n",
    "    X_test_enc.iloc[0, :],\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best_model predicts the sample is 0, which is non-default. And the SHAP plot makes sense that the model raw score is -2.17 which is smaller than base value. The MARRIAGE=2 and LIMIT_BAL=-0.982 are pushing the prediction towards higher score. But the PAY_0=-0.879 is pushing back the prediction to smaller score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model.predict(X_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    test_lgbm_shap_values[1][2, :],\n",
    "    X_test_enc.iloc[0, :],\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best_model predicts the sample is 1, which is default. And the SHAP plot makes sense that the model raw score is 2.01 which is bigger than base value. The PAY_AMT2=-0.225 is pushing back the prediction to smaller score. But the PAY_0=-0.879 are pushing the prediction towards higher score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 13. Summary of results <a name=\"13\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "Imagine that you want to present the summary of these results to your boss and co-workers. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a table summarizing important results. \n",
    "2. Write concluding remarks.\n",
    "3. Discuss other ideas that you did not try but could potentially improve the performance/interpretability . \n",
    "3. Report your final test score along with the metric you used at the top of this notebook in the [Submission instructions section](#si)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_13\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1.\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments = {\n",
    "    \"Dummy\": \"A baseline, low fit time and score time.\",\n",
    "    \"Dummy (recall)\": \"The recall variant of the Dummy model, poor performance.\",\n",
    "    \"logistic_regression\": \"Relatively faster fit time and score time and decent performance.\",\n",
    "    \"logistic_regression (recall)\": \"Not performing well.\",\n",
    "    \"logistic_regression (Best C)\": \"Lower fit time compared to the regular logistic regression model.\",\n",
    "    \"decision tree\": \"relatively longer fit time and overfitting.\",\n",
    "    \"random forest\": \"longest fit time and overfitting.\",\n",
    "    \"LightGBM\": \" relatively faster fit time among theensembles, better generalization.\",\n",
    "}\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "results_df[\"comments\"] = comments.values()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this open-ended mini-project, our focus was on a classification problem: predicting whether a credit card client will default or not. This project draws inspiration from various research conducted in related fields, including a study titled 'The comparisons of data mining techniques for the predictive accuracy of the probability of default of credit card clients' by Cheng Yeh et al. (2008). The objective of these studies is to facilitate credit card debt risk reduction for banking institutions and make credit card financing more accessible to the public.\n",
    "\n",
    "Initially, we employed data visualization and manipulation techniques during exploratory data analysis, preprocessing, and transformations. These methods enabled us to observe data features, generate exploratory ideas, make inferences, and transform the data into formats more suitable for our classification problem.\n",
    "\n",
    "Subsequently, we performed supervised machine learning using several models, including a dummy classifier, LogisticRegression, decision tree, random forest, and LightGBM. We attempted to optimize hyperparameters by utilizing techniques like GridSearchCV and RandomizedSearchCV.\n",
    "\n",
    "To enhance the interpretability of non-linear models, especially gradient boosting machines like LightGBM, we implemented feature importance techniques. These methods prove particularly useful for models that are highly complex and challenging to interpret. Finally, we assessed our model performance using Recall Averaging Macro. We used the recall score because we want to identify positive instances out of all actual positive instances, i.e., we want to focus on identifying fraud cases.\n",
    "\n",
    "Initially, we intended to incorporate CatBoost as one of the models to improve accuracy. However, due to computational resource and time limitations, we were unable to implement CatBoost in this project. Nevertheless, it would have been a valuable addition given its reputation for providing more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe_lgbm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#code from lecture 9\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpipe_lgbm\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      4\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay\u001b[38;5;241m.\u001b[39mfrom_estimator(\n\u001b[0;32m      5\u001b[0m     pipe_lgbm,\n\u001b[0;32m      6\u001b[0m     X_test,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     colorbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe_lgbm' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "#code from lecture 9\n",
    "pipe_lgbm.fit(X_train, y_train)\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_lgbm,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Not default\", \"default\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    "    colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## (Optional) 14. Your takeaway <a name=\"15\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from the supervised machine learning material we have learned so far? Please write thoughtful answers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_14\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLEASE READ BEFORE YOU SUBMIT:** \n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from \"1\" will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "4. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. \n",
    "5. Make sure that the plots and output are rendered properly in your submitted file. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb so that the TAs can view your submission on Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a tricky one but you did it! Have a great weekend! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
